---
title: Install the toolkit on IBM Cloud VPC infrastructure
tabs: ['Quick Install', 'IBM Cloud on VPC', 'IBM Cloud on Classic']
---

import Globals from 'gatsby-theme-carbon/src/templates/Globals';

<PageDescription>

Steps to prepare an environment using the <Globals name="shortName" /> for new <Globals name="ic" />-managed clusters running on VPC infrastructure or existing <Globals name="ic" />-managed
clusters running on classic or VPC infrastructure.

</PageDescription>

See the following for instructions on how to
provision [new <Globals name="ic" />-managed clusters running on classic
infrastructure](./ibm-cloud-on-classic)

<Tabs>

<Tab label="Private Catalog" open="true">

One of the features of the IBM Cloud Catalog is support for private catalog tiles. These can
contain custom Terraform definitions that can accelerate SRE teams in the execution of common
and repetitive tasks. The <Globals name="shortName" /> tools installation can be configured as
a private catalog tile. This is the recommended approach for using this asset multiple times.
This asset enables the easy transition of a default cluster into a cluster that supports
Cloud-Native CI/CD development tools.

### A. Create the catalog

<br></br>

1. Log in to the <Globals name="ic" /> Console
2. Click **Manage->Catalogs** from the top menu
3. Click on **Create Catalog**
4. In the `Create a catalog` dialog, provide the following values:
    - **name:** the name of the catalog, for example `Team Catalog`
    - **description:** (optional) a brief description of the purpose of the catalog
    - **products:** select **Start with no products**
    - **resource group:** click **Update** to change the default resource group for the catalog
5. Click **Create** to complete the catalog creation

### B. Register the <Globals name="shortName" /> tiles in the catalog

<br></br>

<InlineNotification>

**Note:** The following instructions depend on the `jq` command. The information for installing
`jq` can be found on the [<Globals name="shortName" /> prerequisites](/getting-started/prereqs) page

</InlineNotification>

<br />

1. Download `create-catalog-offering.sh` from the <a href="https://github.com/ibm-garage-cloud/ibm-garage-iteration-zero/releases/latest" target="_blank">latest Iteration Zero release</a> and make the file executable
    ```shell script
    LATEST_RELEASE=$(curl -sL https://api.github.com/repos/ibm-garage-cloud/ibm-garage-iteration-zero/releases/latest | jq -r '.tag_name')
    curl -OL "https://github.com/ibm-garage-cloud/ibm-garage-iteration-zero/releases/download/${LATEST_RELEASE}/create-catalog-offering.sh"
    chmod +x create-catalog-offering.sh
    ```

2. Run the `create-catalog-offering.sh` scripts passing in the API Key and the name of the catalog that you created
    ```shell script
    ./create-catalog-offering.sh {API_KEY} "Team Catalog"
    ```

### C. Apply the tile

<br />

Once the tile has been installed it can be used repeatedly to set up clusters with
the CNCF tools. Each time the tile is applied, a set of configuration variables are
required.

1. Log in to the <Globals name="ic" /> Console.
2. Select **Catalog** from the top menu.
3. From the side menu, select your catalog from the drop-down list (e.g. `Team Catalog`). (**IBM Cloud catalog** should be selected initially.)
4. Click **Private** on the side menu to see the private catalog entries
5. Click on the **Cloud-Native Toolkit** tile
6. Enter values for the variables list provided.

    | **Variable**          | **Description**                                                                               | **eg. Value**                 |
    |-----------------------|-----------------------------------------------------------------------------------------------|-------------------------------|
    | `ibmcloud_api_key`    | The API key from IBM Cloud Console that has ClusterAdmin access and supports service creation | `{guid API key from Console}` |
    | `resource_group_name` | Existing resource group in the account where the cluster has been created                     | `dev-team-one`                |
    | `region`              | The region where the cluster has been provisioned                                             | `us-east`, `eu-gb`, etc       |
    | `cluster_exists`      | Flag indicating if the cluster already exists. (`false` means the cluster should be provisioned) | `true` or `false`          |
    | `vpc_cluster`         | Flag indicating that the cluster has been built on VPC infrastructure                         | `true` or `false`             |
    | `vpc_zone_names`      | A comma-separated list of the VPC zones that should be used for worker nodes. This value is requored if `cluster_exists` is set to `false` and `vpc_cluster` is set to `true` | `us-south-1` or `us-east-1,us-east-2` |
    | `cluster_name`        | The name of the cluster (If `cluster_exists` is set to `true` then this name should match an existing cluster) | `dev-team-one-iks-117-vpc` |
    | `cluster_type`        | The type of cluster into which the toolkit will be installed                                  | `kubernetes`, `ocp3`, `ocp4` or `ocp44` |
    | `registry_namespace`  | The namespace that should be used in the IBM Container Registry. If not provided the value will default to the resource group name | `dev-team-one-registry-2020` |
    | `provision_logdna`    | Flag indicating that a new instance of LogDNA should be provisioned                           | `true` or `false`          |
    | `logdna_name`         | The name of the LogDNA instance (If `provision_logdna` is set to `false` this value is used by the scripts to bind the existing LogDNA instance to the cluster) | `cntk-showcase-logdna` |
    | `provision_sysdig`    | Flag indicating that a new instance of Sysdig should be provisioned                           | `true` or `false`          |
    | `sysdig_name`         | The name of the Sysdig instance (If `provision_sysdig` is set to `false` this value is used by the scripts to bind the existing Sysdig instance to the cluster) | `cntk-showcase-sysdig` |

    <InlineNotification>

    **Note:** Provisioning a new cluster using classic infrastructure is not supported with the tile install. In this case
    you are recommended to either create the cluster using the <Globals name="ic" /> Console then use the tile
    to install the Toolkit **OR** to install with one of the other methods.

    </InlineNotification>

<br />


7. Check the box to accept the **Apache 2** license for the tile.
8. Click **Install** to start the install process

This will kick off the installation of the <Globals name="shortName" /> using an
IBM Cloud Private Catalog Tile. The progress can be reviewed from the
**Schematics** entry

</Tab>

<Tab label="Iteration Zero">

### A. Download the Iteration Zero scripts

<br />


1. Clone the [ibm-garage-iteration-zero](https://github.com/ibm-garage-cloud/ibm-garage-iteration-zero) Git repository to your local filesystem
    ```shell script
    git clone git@github.com:ibm-garage-cloud/ibm-garage-iteration-zero.git
    ```

### B. Configure the credentials

<br />

1. In a terminal, change to the `ibm-garage-iteration-zero` cloned directory
    ```shell script
    cd ibm-garage-iteration-zero
    ```

2. Copy the `credentials.template` file to a file named `credentials.properties`
    ```shell script
    cp credentials.template credentials.properties
    ```

   **Note:** `credentials.properties` is already listed in `.gitignore` to prevent the
   private credentials from being committed to the git repository

3. Update the value for the `ibmcloud.api.key` property in `credentials.properties` with your <Globals name="ic" /> API key

    **Note:** The API key should have been set up during [plan installation](/getting-started-0/plan-installation/ibm-cloud).

### C. Configure the environment variables

<br />

The settings for creating the <Globals name="shortName" /> on <Globals name="ic" /> are set in the `environment-ibmcloud.tfvars`
file in the `./terraform/settings` directory of the `ibm-garage-iteration-zero` repository.

There are a number of values that can be applied in the file, some required and some optional. Consult with
the following table to determine which values should be used:

| **Variable**          | **Required?** | **Description**                                                                     | **eg. Value**                 |
|-----------------------|-----|-----------------------------------------------------------------------------------------------|-------------------------------|
| `cluster_type`        | yes | The type of cluster into which the toolkit will be installed                                  | `kubernetes`, `ocp3`, `ocp4` or `ocp44` |
| `cluster_exists`      | yes | Flag indicating if the cluster already exists. (`false` means the cluster should be provisioned) | `true` or `false`          |
| `resource_group_name` | yes | Existing resource group in the account where the cluster has been created                     | `dev-team-one`                |
| `vpc_cluster`         | yes | Flag indicating that the cluster has been built on VPC infrastructure. Defaults to `true`     | `true` or `false`             |
| `name_prefix`         | no  | The prefix that should be applied for any resources that are provisioned. Defaults to `{resource_group_name}` | `dev-one`     |
| `region`              | no  | The region where the cluster has been/will be provisioned                                     | `us-east`, `eu-gb`, etc       |
| `vpc_zone_names`      | no  | A comma-separated list of the VPC zones that should be used for worker nodes. This value is required if `cluster_exists` is set to `false` and `vpc_cluster` is set to `true` | `us-south-1` or `us-east-1,us-east-2` |
| `cluster_name`        | no  | The name of the cluster (If `cluster_exists` is set to `true` then this name should match an existing cluster). Defaults to `{prefix_name}-cluster` or `{resource_group_name}-cluster` | `dev-team-one-iks-117-vpc` |
| `registry_namespace`  | no  | The namespace that should be used in the IBM Container Registry. Defaults to `{resource_group_name}` | `dev-team-one-registry-2020` |
| `provision_logdna`    | no  | Flag indicating that a new instance of LogDNA should be provisioned. Defaults to `false`      | `true` or `false`          |
| `logdna_name`         | no  | The name of the LogDNA instance (If `provision_logdna` is set to `false` this value is used by the scripts to bind the existing LogDNA instance to the cluster) | `cntk-showcase-logdna` |
| `provision_sysdig`    | no  | Flag indicating that a new instance of Sysdig should be provisioned. Defaults to `false`      | `true` or `false`          |
| `sysdig_name`         | no  | The name of the Sysdig instance (If `provision_sysdig` is set to `false` this value is used by the scripts to bind the existing Sysdig instance to the cluster) | `cntk-showcase-sysdig` |

<br />

Update `environment-ibmcloud.tfvars` with the appropriate values for your installation.

### D. (Optional) Customize the installed components

<br />

The `terraform/stages` directory contains the default set of stages that define the
modules that will be applied to the environment. The stages can be customized to change
the makeup of the environment that is provisioned by either removing or adding stages from/to the
`terraform/stages` directory.

**Note:** The stages occasionally have dependencies on other stages (e.g. most all
depend on the cluster module, many depend on the namespace module, etc.) so be aware of those
dependencies as you start making changes. Dependencies are reflected in the `module.{stage name}` references
in the stage variable list.

The `terraform/stages/catalog` directory contains some optional
stages that are prep-configured and can be dropped into the `terraform/stages` directory. Other
modules are available from the [Garage Terraform Modules](https://github.com/ibm-garage-cloud/garage-terraform-modules)
catalog and can be added as stages to the directory as well. Since this is Terraform,
any other Terraform scripts and modules can be added to the `terraform/stages` directory
as desired.

### E. Run the installation

<br />

1. Open a terminal to the `ibm-garage-iteration-zero` directory
2. Launch a [Developer Tools Docker container](https://github.com/ibm-garage-cloud/ibm-garage-cli-tools "Cloud Garage Tools Docker image") from which the Terraform scripts will be run
    ```shell script
    ./launch.sh
    ```

    This will download the Cloud Garage Tools Docker image that contains all the necessary tools to execute Terraform scripts
    and exec shell into the running container. When the container starts it
    mounts the filesystem's `./terraform/` directory as `/home/devops/src/` and loads the values from the
    `credentials.properties` file as environment variables.

3. Apply the Terraform by running the provided `runTerraform.sh` script

    ```shell script
    ./runTerraform.sh
    ```

    This script collects the values provided in the `environment-ibmcloud.tfvars` and the
    stages defined in the `terraform/stages` to build the Terraform workspace. Along the way it
    will prompt for a couple pieces of information.

    1. Type of installation: `ibmcloud` or `ocp`

        There are two major paths to installing with the Toolkit. In this case, we are installing
        into an <Globals name="ic" />-managed environment so we will select `ibmcloud`.

        This prompt can be skipped by providing `--ibmcloud` as an argument to `./runTerraform.sh`

    2. Handling of an old workspace (if applicable): `keep` or `delete`

        If you executed the script previously for the current cluster configuration and the workspace directory still
        exists then you will be prompted to either keep or delete the workspace directory. Keep the workspace directory if
        you want to use the state from the previous run as a starting point to either add or remove configuration. Delete
        the workspace if you want to start with a clean install of the Toolkit.

        This prompt can be skipped by providing `--delete` or `--keep` as an argument to `./runTerraform.sh`

    3. Verify the installation configuration

        The script will verify some basic settings and prompt if you want to proceed. After you select **Y** (for yes),
        the Terraform Apply process will begin to create the infrastructure and services for your environment.

        This prompt can be skipped by providing `--auto-approve` as an argument to `./runTerraform.sh`

    Creating a new cluster takes about 1.5 hours on average (but can also take considerably longer)
    and the rest of the process takes about 30 minutes.

</Tab>

</Tabs>

## Post-installation steps

Once the Terraform has completed, walk through the [post installation steps](/getting-started-day-0/post-installation) to
explore and verify the environment.

## Troubleshooting

If you find that the Terraform provisioning has failed, for Private Catalog delete the workspace and for Iteration Zero  try re-running the `runTerraform.sh` script again.
The state will be saved and Terraform will try and apply the configuration to match the desired end state.

If you find that some of the services have failed to create in the time allocated, try the following with Iteration zero:

1. Manually delete the service instances in your resource group
3. Re-run the `runTerraform.sh` script with the `--delete` argument to clean up the state
    ```shell script
    ./runTerraform.sh --delete
    ```
